---
title: "17 - Linear regression models"
output: github_document
---



```{r setup, include=FALSE}
library(knitr)
library(rmdformats)
library(tidyverse)
# figs folder
if (!file.exists("figs")) {
  dir.create("figs")
}
# chunk options
knitr::opts_chunk$set(
  echo = TRUE, # show/hide all code
  results = "hide", # hide results
  tidy = FALSE, # cleaner code printing
  comment = "#> ", # better console printing
  eval = TRUE, # turn this to FALSE stop code chunks from running
  message = TRUE, # show messages
  warning = FALSE, # show warnings
  size = "small", # size of the text
  fig.path = "figs/", # location of files
  fig.height = 5, # height of figures
  fig.width = 7
) # width of figures
# knit options
knitr::opts_knit$set(
  width = 78,
  progress = FALSE
)
# base options
base::options(
  tibble.print_max = 25,
  tibble.width = 78,
  max.print = "1000",
  scipen = 100000000
)
```

Load packages below. 

```{r packages, message=FALSE, warning=FALSE}
library(magrittr) # Pipes %>%, %T>% and equals(), extract().
library(tidyverse) # all tidyverse packages
library(fs) # file management functions
library(fivethirtyeight)
library(mdsr) # modern data science with R
library(broom) # tidying models
library(modelr) # modelr package
library(ggrepel) # repel text overlay in graphs
library(gridExtra) # arrange multiple plots
library(grid) # text for graph title
library(egg) # for ggarrange
library(corrr) # correlations
# devtools::install_github("ropensci/skimr", ref = "v2")
library(skimr)
library(hrbrthemes)
```

# Linear models 

Linear regression is one of the most common models used in statistics. They're an excellent point of entry into statistical inference because they're relatively easy to graph and visualize. Linear relationships are pretty intuitive, too. You've probably thought of two measurements as a having a linear relationship and didn't realize it. In this post, we will cover the essential elements of linear regression models and how to use them to make predictions.

## What is a model?

In statistics and data science, the term 'model' is used to describe our best attempt to represent some phenomena with numbers and math. In the same way that LEGO models provide children a smaller version of reality to play with, statistical models give us the ability to think about relationships between things in the real world. 

These models give us the ability to make predictions about how the world works, which is always valuable. For example, Amazon uses machine learning algorithms to provide you an estimate of when your order arrives on your doorstep. Netflix and YouTube use data and statistics to determine and recommend what movies and shows you'll enjoy.

It's crucial always to remember that counting and measurement are tricky, so the models built on those numbers should be interpreted and applied with caution. We need to remember statistical models are toy versions of reality. That fact becomes evident when--despite the massive amounts of data we have--a model fails to make an accurate prediction (and your Prime order shows up two days late).  

If you are wondering why there is so much dependence on statistical modeling when it's clear they can still lead us astray? Well, think back to the LEGOs: children learn a lot about reality by building LEGO models. In particular, they learn to experiment, and how to implement trial and error, two major components of science. The quote below summarizes these aspects quite well,

> "Science is the driving component behind all creations that a child has. The foundation of science is to come up with an idea and to prove it practically. LEGOs do this naturally through imagination. A child simply comes up with an idea then develops it." - [Preschool inspirations](https://preschoolinspirations.com/10-important-skills-children-learn-legos/)

Statistical models are a direct analog of the lessons above. Someone comes up with an idea, develops the concept into something they can measure, then they use a model to see how well their idea holds up. More importantly, statistical models don't have to *prove* anything to help us understand the world around us. Consider the wise words from George Box, 

> "Models, of course, are never true, but fortunately, it is only necessary that they be useful." - George Box, [Some Problems of Statistics and Everyday Life](https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481600)

Knowing which models *aren't* a good representation of reality is as essential for advancing science as discovering the useful models. As Karl Popper, the philosopher of science emphasized, there are only a few ways to get the right model and an infinite number of ways to get it wrong. Identifying the incorrect model clarifies our thinking and helps us move forward towards a better understanding. Building and comparing the results from multiple models allow the best to rise to the top, and those models that remain after many experiments often turn out to be quite reliable.

## Visualizing relationships

We will start by visualizing a relationship between two variables. Before looking at actual data, we should start by getting an idea of what we should expect to see. For example, let's consider a headline from the data journalism site Fivethirtyeight published in 2017 titled, ["Higher Rates Of Hate Crimes Are Tied To Income Inequality."](https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/). 

That headline might give the impression that the article contains anecdotal information about two fuzzy concepts. But the language is giving us hints about what the graph of these two measurements might look like. For instance, "*Higher Rates Of...*" and "*Are Tied To...*" sounds like "*When this number is high*" and  "*so is this number*" So if we drew a graph of these two measurements with an `x` and `y` coordinate, it might look like the figure below. 

```{r 01-linear-regression-sketch, results='asis', echo=FALSE}
knitr::include_graphics(path = "images/17.1-linear-regression-sketch-01.png")
```

By convention, we typical call the measurement we're trying to predict the **outcome** or **response** variable, and put it on the `y` axis. The variables we'll use for explanation or prediction are typically called (you guessed it) **explanatory** or **predictor** variables, and we place these numbers on the `x` axis. We agree with [Julian J. Faraway](https://people.bath.ac.uk/jjf23/) and avoid using the terms *independent* or *dependent* unless we've designed and experiment, but we will discuss this topic more in a later post.

These could also be linear, but in the other direction: 

```{r 01-linear-regression-sketch-2, results='asis', echo=FALSE}
knitr::include_graphics(path = "images/17.1-linear-regression-sketch-02.png")
```

Generally, the important part about linear relationships is what the line represents. Consider the simulated data below:




when we graph the outcome and predictor, and draw a line that passes through the mean of the outcome variable.

and the number of points above and below this line indicate how well the explanatory variable predicts or explains.

can draw a straight line through the center of the points. But not all relationships are linear, and it's important to know what a non-linear relationship looks like because assuming two variables have a linear relationship without visualizing it can get us in trouble. A non-linear relationship might look like the graph below:

```{r 01-non-linear-regression-sketch, results='asis', echo=FALSE}
knitr::include_graphics(path = "images/17.1-non-linear-regression-sketch.png")
```


 

## YouTube comedy sketch data 

Now that we have an idea for what a linear relationship between these two variable might look like, we can start graphing the data and see if it meets our expectations. The data If we read the [README](https://github.com/richardcornish/sketch-comedy-data), we discover both datasets have the following variables:

<!--
```json
   "statistics": {
    "viewCount": "3057",
    "likeCount": "25",
    "dislikeCount": "0",
    "favoriteCount": "17",
    "commentCount": "12"
}
```
-->

- `published_at` = date video was published 
- `dislike_count` = number of dislikes 
- `url` = url for video  
- `id` = id for video 
- `comment_count` = number of comments per video 
- `title` = title of video 
- `view_count` = number of views per video 
- `like_count` = number of likes per video 



We will download and import the data into our workspacein the code chunk below:

```{r import-comedy-data-github, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
source("code/17.1-import.R")
```

We can check for the objects using `ls()` to make sure they imported.

```{r check-import}
ls()
```


## Wrangling

These data don't need much wrangling, but I like to break out wrangling into two separate tasks: things I do that change or create entire data set(s), and 2) things I do that change or create new variables. I typically have a few variables in mind when I start building visualizations or models, but this always takes a few steps to get the variables or data exactly how I want them.

If I frame it this way, it forces me to think about the global changes that need to happen for me to create the variables I need. Below are the wrangling steps I perform on the two data sets: 1) remove the Private videos (there are no metrics for these), 2) then create a new variable that helps me identify the `comedian`. The final step and format the variable names.  

```{r tidy-import-data, message=FALSE, warning=FALSE, results='hide', include=FALSE, echo=TRUE}
source("code/17.2-tidy.R")
source("code/17.3-wrangle.R")
```


```{r AmySchumer}
AmySchumer %>% dplyr::glimpse(78)
```

```{r KeyAndPeele}
KeyAndPeele %>% dplyr::glimpse(78)
```

I added the following variable to keep track of who's data we're looking at:

- `comedian` = comedian name 

```{r YouTubeComedyData}
YouTubeComedyData %>% dplyr::count(comedian)
```

## Visualizing relationships: YouTube views vs. likes 

I love comedy, and YouTube has tons of short clips from my favorite comedians. I'm curious about the relationship between the number of times a video is viewed, and how that relates to the number of times a video receives a 'like' (or thumbs up). Why would I assume this relationship is linear? Good question. I think this might be a linear relationship because everyone can only like a YouTube video once, and my personal assumption is most people who view comedy skits are in a happy mood, so they'll be more likely to 'like' the videos that made them laugh. 

So we've loaded data from two comedy acts (technically three people, so I'll be referring to each data set as representing an 'act') from YouTube. All of the sketch acts stored in `YouTubeComedyData` data set, and we also have a data set for each act: `AmySchumer` and `KeyAndPeele`. 

We will start by looking to see if the `YouTubeComedyData` data can help us figure out if there is a linear relationship between views and likes. 

```{r 01-plot_Views_Likes}
plot_Views_Likes <- YouTubeComedyData %>% 
    ggplot2::ggplot(aes(
     # views 
    x = view_count,
     # likes
    y = like_count
  )) +
  ggplot2::geom_point(
    size = 2,
    alpha = 6 / 10,
    show.legend = FALSE
  ) +
      # add titles
  ggplot2::labs(
    x = "Views of Amy Schumer and Key & Peele comedy sketches on YouTube",
    y = "Likes of Amy Schumer and Key & Peele comedy sketches on YouTube"
  )
plot_Views_Likes
```

The relationship looks linear, but this graph isn't showing us both sets of comedian data very well. We'll plot each artist independently to clear up some of the over-plotting. 

```{r 02-plot_AmySchumerViewVsLikes}
# start with labels
amys_labs <- ggplot2::labs(
        title = "Views VS Likes of Amy Schumer's YouTube Videos",
            x = "Views of Amy Schumer comedy sketches",
            y = "Likes of Amy Schumer comedy sketches") 

plot_AmySchumerViewVsLikes <- AmySchumer %>%  
  ggplot2::ggplot(aes(
     # views 
    x = view_count,
     # likes
    y = like_count
  )) +
  ggplot2::geom_point(
    size = 2,
    alpha = 6 / 10,
    color = "darkred",
    show.legend = FALSE
  ) +
  # add titles
    hrbrthemes::scale_fill_ipsum() + 
    hrbrthemes::theme_ipsum_rc(base_family = "EnvyCodeR", 
                               base_size = 12,
                               axis_title_size = 15,
                               plot_title_size = 18,
                               plot_title_family = "Menlo-Regular") +
    ggplot2::theme(axis.text.x = element_text(hjust = c(0, 0.5, 0.5, 0.5, 1))) +
    ggplot2::theme(legend.position = "Bottom") + 
    amys_labs
plot_AmySchumerViewVsLikes
```

```{r 03-plot_KeyAndPeeleViewVsLikes}
# start with labs
kandp_labs <- ggplot2::labs(
  title = "Views VS Likes of Key & Peele's YouTube Videos",
    x = "Views of Key and Peele comedy sketches on YouTube",
    y = "Likes of Key and Peele comedy sketches on YouTube")
# plot 2
plot_KeyAndPeeleViewVsLikes <- KeyAndPeele %>%
  ggplot2::ggplot(aes(
      # views 
    x = view_count,
      # likes
    y = like_count
  )) +
  ggplot2::geom_point(
    size = 2,
    alpha = 6 / 10,
    color = "royalblue",
    show.legend = FALSE
  ) +
  hrbrthemes::scale_color_ipsum() +
  hrbrthemes::scale_fill_ipsum() +
  hrbrthemes::theme_ipsum_tw(base_size = 12,
                             axis_title_size = 14, 
                             plot_title_size = 22) +
  ggplot2::theme(legend.position = "none") +
  # add labs
  kandp_labs
plot_KeyAndPeeleViewVsLikes
```

```{r maxes, echo=FALSE, include=FALSE, eval=TRUE}
amy_max_view_count <- max(AmySchumer[ , "view_count"], na.rm = TRUE)
kp_max_view_count <- max(KeyAndPeele[ , "view_count"], na.rm = TRUE)
```

This is better because we can see a separate plot for each comedic act, but we still have the issue of outliers. The relationship between views and likes certainly looks linear for both sets of videos, but the maximum values for `view_count` is making it hard to read. The max `view_count` for `Amy Schumer` is `r amy_max_view_count`, and the max `view_count` for `Key & Peele` is `r kp_max_view_count`. Including these max values creates a ton of empty chart space. 

Let's take a look at the highest views for each video by comedic act, and label those on the graph. 

```{r set-theme}
# set the theme to theme_light
ggplot2::theme_set(theme_ipsum_tw(base_family = "Titillium Web",
                         base_size = 12, 
                         plot_title_family = "Poppins",
                         axis_title_size = 15))
```


```{r 03-plot_AmySchumerViewVsLikesV2}
# first we create some data sets for the top views and likes 
TopAmyViews <- AmySchumer %>% dplyr::filter(view_count >= 3000000 | 
                                                like_count >= 30000)
plot_AmySchumerViewVsLikesV2 <- AmySchumer %>%  
  ggplot2::ggplot(aes(
     # views 
    x = view_count,
     # likes
    y = like_count
  )) +
  ggplot2::geom_point(
    size = 2,
    alpha = 6 / 10,
    show.legend = FALSE
  ) + 
  # add labels to states
  geom_text_repel(
    data = TopAmyViews, 
    aes(label = title),
    # size of text
    size = 3.5,
    # opacity of the text
    alpha = 7 / 10,
    # in case there is overplotting
    arrow = arrow(
      length = unit(0.05, "inches"),
      type = "open",
      ends = "last"
    ),
    show.legend = FALSE
  ) +
    # add some color for the points
  geom_point(
    data = TopAmyViews, 
    aes(color = title, 
        label = title), 
    show.legend = FALSE) + 
  # add titles
  amys_labs
plot_AmySchumerViewVsLikesV2
```

```{r 04-plot_KeyAndPeeleViewVsLikesV2}
# repeat this for K&P
TopKandPViews <- KeyAndPeele %>% dplyr::filter(view_count >= 30000000 | 
                                                   like_count >= 200000)
plot_KeyAndPeeleViewVsLikesV2 <- KeyAndPeele %>%  
  ggplot2::ggplot(aes(
     # views 
    x = view_count,
     # likes
    y = like_count
  )) +
  ggplot2::geom_point(
    size = 2,
    alpha = 6 / 10,
    show.legend = FALSE
  ) + 
  # add labels to states
  geom_text_repel(
    data = TopKandPViews, 
    aes(label = title),
    # size of text
    size = 3.5,
    # opacity of the text
    alpha = 7 / 10,
    # in case there is overplotting
    arrow = arrow(
      length = unit(0.05, "inches"),
      type = "open",
      ends = "last"
    ),
    show.legend = FALSE
  ) +
  geom_point(
    data = TopKandPViews, 
    aes(color = title, 
        label = title), 
    show.legend = FALSE) + 
  # add titles
kandp_labs
plot_KeyAndPeeleViewVsLikesV2
```

Let's remove these outliers for now, but we'll revisit the outliers later.  
+ For `Amy Schumer`, we will focus on the YouTube data with a `view_count` of less than `3,000,000` and a `like_count` of less than `30,000`.  
+ For the `Key & Peele`, we will be looking at YouTube data with less than `30,000,000` views `200,000`. 
                                                
```{r 04-plot_AmySchumerSmall}
amy_small_labs <- ggplot2::labs(
    title = "Views VS Likes of Amy Schumer's YouTube Videos",
    subtitle = "Limited to < 3,000,000 Views and < 30,000 Likes",
    x = "Views of comedy sketches on YouTube",
    y = "Likes of comedy sketches on YouTube")
AmySchumerSmall <- AmySchumer %>% 
    dplyr::filter(view_count < 3000000 & like_count < 30000)
plot_AmySchumerSmall <- AmySchumerSmall %>% 
    ggplot2::ggplot(aes(
     # views 
    x = view_count,
     # likes
    y = like_count)) + 
    # points
    ggplot2::geom_point(
        # change opacity
        alpha = 6/10) + 
    # labels
    amy_small_labs
plot_AmySchumerSmall
```

```{r plot_KeyAndPeeleSmall}
kandp_small_labs <- ggplot2::labs(
    title = "Views VS Likes of Key & Peele's YouTube Videos",
    subtitle = "Limited to < 30,000,000 Views and < 200,000 Likes",
    x = "Views of comedy sketches on YouTube",
    y = "Likes of comedy sketches on YouTube")
KeyAndPeeleSmall <- KeyAndPeele %>% 
    dplyr::filter(view_count < 30000000 & like_count < 200000)
plot_KeyAndPeeleSmall <- KeyAndPeeleSmall %>% 
    ggplot2::ggplot(aes(
     # views 
    x = view_count,
     # likes
    y = like_count)) + 
    # points
    ggplot2::geom_point(
        # change opacity
        alpha = 6/10) + 
    # labels
    kandp_small_labs
plot_KeyAndPeeleSmall
```

Great! Now we can start understanding the linear relationship between these variables. 

## Looking for correlations 

The [`corrr` package](https://cran.r-project.org/web/packages/corrr/index.html) gives us the ability to examine the correlation between to variables we assume are related to one another. These are part of the [`tidymodels` package](https://github.com/tidymodels/corrr), a meta package for modeling in the `tidyverse`.

We will use `dplyr::select()` to get the two variables we're interested in, `view_count` and `like_count`, then pass these to the `corrr::correlate()` function. A Pearson correlation  is the best option here because 

```{r}
AmySchumerSmall %>% 
  dplyr::select(view_count, like_count) %>%
  corrr::correlate(method = "pearson", quiet = TRUE) 
```

We can reduce these two only the  